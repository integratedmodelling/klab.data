{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crops / Monetary / Deflator calculation\n",
    "\n",
    "With this code what are we going to do is extract the data from a website and modelate it for every country in a global Scale. There are three tables to operate.\n",
    "Production, Deflation and Monetary. Once we have finished with them, we do the final operation and export it as csv\n",
    "\n",
    "In the future, we want to improve this code, by pointing to a table, like the lookup table, \n",
    "\n",
    "Data source: [FAO](http://www.fao.org/faostat/en/#data/QC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the libraries\n",
    "import requests\n",
    "import pandas as pd\n",
    "import zipfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#This part is for the parsing of the data inside. It did not work jet. We will pass it for now\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "\n",
    "url = 'Request URL: http://fenixservices.fao.org/faostat/api/v1/en/data/QC?area=11&area_cs=FAO&element=2312&item=515&item_cs=FAO&year=1972&show_codes=true&show_unit=true&show_flags=true&null_values=false&page_number=1&page_size=100&output_type=objects'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crops production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the file parameters\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "\n",
    "#Make sure that the URL works, since it has changed one time\n",
    "#old_url_production=\"http://fenixservices.fao.org/faostat/static/bulkdownloads/Production_Crops_E_All_Data.zip\"\n",
    "\n",
    "url_production = 'http://fenixservices.fao.org/faostat/static/bulkdownloads/Production_Crops_Livestock_E_All_Data.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name                                             Modified             Size\n",
      "Production_Crops_Livestock_E_All_Data.csv      2021-05-26 12:37:24     77500682\n",
      "Production_Crops_Livestock_E_All_Data_NOFLAG.csv 2021-05-26 12:37:34     60777983\n",
      "Production_Crops_Livestock_E_Flags.csv         2021-05-26 12:50:10          245\n"
     ]
    }
   ],
   "source": [
    "#Open and download the file\n",
    "os.chdir(r\"C:\\Users\\ruben.crespo\\Downloads\") #C:\\Users\\ruben.crespo\\Documents\\03_tests\\FAO\n",
    "zip_file = requests.get(url_production, headers=headers)\n",
    "with open(\"Production_Crops_Livestock_E_All_Data_NOFLAG.zip\", \"wb\") as file:\n",
    "    file.write(zip_file.content)\n",
    "    \n",
    "#print all the files  inside the zip to choose which one do we want\n",
    "with zipfile.ZipFile(\"Production_Crops_Livestock_E_All_Data_NOFLAG.zip\") as zip:\n",
    "    zip.printdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#info Source: https://samukweku.github.io/data-wrangling-blog/python/pandas/compressed%20data/zip/archived%20data/2020/07/21/Extract-DataFrame-from-Compressed-Data-into-Pandas.html\n",
    "#Open the table\n",
    "with zipfile.ZipFile(\"Production_Crops_Livestock_E_All_Data_NOFLAG.zip\") as zip:\n",
    "    data = zip.open(\"Production_Crops_Livestock_E_All_Data_NOFLAG.csv\") #open the zip file\n",
    "df_p = pd.read_csv(data, encoding= \"ISO-8859-1\", header=0) #The encoding was giving me problems\n",
    "\n",
    "#Transform the table\n",
    "df_p = df_p.drop([\"Area Code\", \"Item Code\", \"Element Code\"], axis = 1) #axis 1 for columns 0 for rows\n",
    "df_p = df_p[df_p['Element'].str.match('Production')] #take rows with the \"production value\" from the \"Element\" column\n",
    "df_p = df_p.drop([\"Element\", \"Unit\"], axis = 1) #Delete columns\n",
    "\n",
    "df_p = df_p.reset_index(drop=True)  #reset index and delete old index\n",
    "df_p = df_p.rename(columns={'Item': ''}) #take out the Item part as is giving problems\n",
    "df_p.set_index(['Area', ''], inplace=True) #we create from the dataframe a multiindex with these two columns\n",
    "df_p.sort_index(inplace=True) #we sort the data\n",
    "df_p = df_p.stack() #take the header of the production alements and make a column with it\n",
    "\n",
    "\n",
    "df_p = df_p.unstack(1) #take the 2nd column and make the header \n",
    "# df_p = df_p.rename(columns={'Item': None})\n",
    "df_p = df_p.reset_index(level=[0,1])    #multiindex to single index, indicate the columns\n",
    "df_p['level_1'] = df_p['level_1'].map(lambda x: x.lstrip('Y')) #remove the letter Y from year\n",
    "df_p.rename(columns={'level_1': 'Year'}, inplace=True) #rename column for year\n",
    "\n",
    "df_p\n",
    "\n",
    "\"\"\"filter the countries\"\"\"\n",
    "\n",
    "Belgium_Luxembourg = (df_p['Area'] == 'Belgium-Luxembourg') & (df_p['Year'] < '2000') #create a filter and use it the the dataframe. En caso de que vaya lento, hacer un filtro más preciso\n",
    "df_Belgium_Luxembourg = df_p.loc[Belgium_Luxembourg] #create copy dataframe out of the filter\n",
    "df_Belgium_Luxembourg.set_index(['Area', 'Year'], inplace=True) #we create a multiindex to do the operation\n",
    "df_Belgium = df_Belgium_Luxembourg * 0.97 #we do the operatiion\n",
    "df_Belgium = df_Belgium.reset_index(level=[0,1])  #we undo the multyindex\n",
    "df_Belgium['Area'].replace({'Belgium-Luxembourg': 'Belgium'}, inplace=True)\n",
    "df_p.append(df_Belgium, ignore_index=True)\n",
    "\n",
    "df_Luxembourg = df_Belgium_Luxembourg * 0.03 #we do the operatiion\n",
    "df_Luxembourg = df_Luxembourg.reset_index(level=[0,1])  #we undo the munltiindex\n",
    "df_Luxembourg['Area'].replace({'Belgium-Luxembourg': 'Luxembourg'}, inplace=True)\n",
    "df_p = df_p.append(df_Luxembourg, ignore_index=True)\n",
    "\n",
    "df_p = df_p[df_p.Area != 'Belgium-Luxembourg']\n",
    "\n",
    "Czechoslovakia = (df_p['Area'] == 'Czechoslovakia') & (df_p['Year'] < '1993') #create a filter and use it the the dataframe\n",
    "df_Czechoslovakia = df_p.loc[Czechoslovakia] #create dataframe out of the filter\n",
    "df_Czechoslovakia.set_index(['Area', 'Year'], inplace=True) #we create a multiindex to do the operation\n",
    "df_Czechia = df_Czechoslovakia * 0.69 #we do the operatiion\n",
    "df_Czechia = df_Czechia.reset_index(level=[0,1])  #we undo the munltiindex\n",
    "df_Czechia['Area'].replace({'Czechoslovakia': 'Czechia'}, inplace=True)\n",
    "df_p.append(df_Czechia, ignore_index=True)\n",
    "\n",
    "df_Slovakia = df_Czechoslovakia * 0.31 #we do the operatiion\n",
    "df_Slovakia = df_Slovakia.reset_index(level=[0,1])  #we undo the munltiindex\n",
    "df_Slovakia['Area'].replace({'Czechoslovakia': 'Slovakia'}, inplace=True)\n",
    "df_p = df_p.append(df_Slovakia, ignore_index=True)\n",
    "\n",
    "df_p = df_p[df_p.Area != 'Czechoslovakia']\n",
    "\n",
    "Serbia_and_Montenegro = (df_p['Area'] == 'Serbia and Montenegro') & (df_p['Year'] < '2006') #create a filter and use it the the dataframe\n",
    "df_Serbia_and_Montenegro = df_p.loc[Serbia_and_Montenegro] #create dataframe out of the filter\n",
    "df_Serbia_and_Montenegro.set_index(['Area', 'Year'], inplace=True) #we create a multiindex to do the operation\n",
    "df_Serbia = df_Serbia_and_Montenegro * 0.94 #we do the operatiion\n",
    "df_Serbia = df_Serbia.reset_index(level=[0,1])  #we undo the munltiindex\n",
    "df_Serbia['Area'].replace({'Serbia and Montenegro': 'Serbia'}, inplace=True)\n",
    "df_p = df_p.append(df_Serbia, ignore_index=True)\n",
    "\n",
    "df_Montenegro = df_Serbia_and_Montenegro * 0.06 #we do the operatiion\n",
    "df_Montenegro = df_Montenegro.reset_index(level=[0,1])  #we undo the munltiindex\n",
    "df_Montenegro['Area'].replace({'Serbia and Montenegro': 'Montenegro'}, inplace=True)\n",
    "df_p = df_p.append(df_Montenegro, ignore_index=True)\n",
    "\n",
    "df_p = df_p[df_p.Area != 'Serbia and Montenegro']\n",
    "\n",
    "Ethiopia_PDR = (df_p['Area'] == 'Ethiopia PDR') & (df_p['Year'] < '1993') #create a filter and use it the the dataframe\n",
    "df_Ethiopia_PDR = df_p.loc[Ethiopia_PDR] #create dataframe out of the filter\n",
    "df_Ethiopia_PDR.set_index(['Area', 'Year'], inplace=True) #we create a multiindex to do the operation\n",
    "df_Eritrea = df_Ethiopia_PDR * 0.04 #we do the operatiion\n",
    "df_Eritrea = df_Eritrea.reset_index(level=[0,1])  #we undo the munltiindex\n",
    "df_Eritrea['Area'].replace({'Ethiopia PDR': 'Eritrea'}, inplace=True)\n",
    "df_p = df_p.append(df_Eritrea, ignore_index=True)\n",
    "\n",
    "df_Ethiopia = df_Ethiopia_PDR * 0.96 #we do the operatiion\n",
    "df_Ethiopia = df_Ethiopia.reset_index(level=[0,1])  #we undo the munltiindex\n",
    "df_Ethiopia['Area'].replace({'Ethiopia PDR': 'Ethiopia'}, inplace=True)\n",
    "df_p = df_p.append(df_Ethiopia, ignore_index=True)\n",
    "\n",
    "df_p = df_p[df_p.Area != 'Ethiopia PDR']\n",
    "\n",
    "Sudan_former = (df_p['Area'] == 'Sudan (former)') & (df_p['Year'] < '2011') #create a filter and use it the the dataframe\n",
    "df_Sudan_former = df_p.loc[Sudan_former] #create dataframe out of the filter\n",
    "df_Sudan_former.set_index(['Area', 'Year'], inplace=True) #we create a multiindex to do the operation\n",
    "df_Sudan = df_Sudan_former * 1 #we do the operatiion\n",
    "df_Sudan = df_Sudan.reset_index(level=[0,1])  #we undo the munltiindex\n",
    "df_Sudan['Area'].replace({'Sudan (former)': 'Sudan'}, inplace=True)\n",
    "df_p = df_p.append(df_Sudan, ignore_index=True)\n",
    "\n",
    "df_South_Sudan = df_Sudan_former * 0 #we do the operatiion\n",
    "df_South_Sudan = df_South_Sudan.reset_index(level=[0,1])  #we undo the munltiindex\n",
    "df_South_Sudan['Area'].replace({'Sudan (former)': 'South Sudan'}, inplace=True)\n",
    "df_p = df_p.append(df_South_Sudan, ignore_index=True)\n",
    "\n",
    "df_p = df_p[df_p.Area != 'Sudan (former)']\n",
    "\n",
    "Indonesia = (df_p['Area'] == 'Indonesia') & (df_p['Year'] < '2011') #create a filter and use it the the dataframe\n",
    "df_Indonesia_main = df_p.loc[Indonesia] #create dataframe out of the filter\n",
    "df_Indonesia_main.set_index(['Area', 'Year'], inplace=True) #we create a multiindex to do the operation\n",
    "df_Timor_Leste = df_Indonesia_main * 0.001 #we do the operatiion\n",
    "df_Timor_Leste = df_Timor_Leste.reset_index(level=[0,1])  #we undo the munltiindex\n",
    "df_Timor_Leste['Area'].replace({'Indonesia': 'Timor Leste'}, inplace=True)\n",
    "df_p = df_p.append(df_Timor_Leste, ignore_index=True)\n",
    "\n",
    "\n",
    "df_Indonesia = df_Indonesia_main * 0.999 #we do the operatiion\n",
    "df_Indonesia = df_Indonesia.reset_index(level=[0,1])  #we undo the multyindex\n",
    "df_p = df_p[df_p.Area != 'Indonesia'] #we delete the original Indonesia\n",
    "df_p = df_p.append(df_Indonesia, ignore_index=True) #we import the new Indonesia\n",
    "\n",
    "\n",
    "\"\"\"Prepare the data to operate\"\"\"\n",
    "df_p = df_p.sort_values(by=['Area', 'Year'])\n",
    "df_p = df_p.reset_index(drop=True)  #reset index and delete old index\n",
    "\n",
    "#since we want to do all in python, we prepare the table for the final operation as a multyindex\n",
    "df_p.set_index(['Area', 'Year'], inplace=True)\n",
    "df_p\n",
    "\n",
    "#the table is ready to be exported as csv (just in case)\n",
    "df_p.to_csv('production_data_1.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deflator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the file parameters\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "\n",
    "url_deflator = 'http://fenixservices.fao.org/faostat/static/bulkdownloads/Deflators_E_All_Data.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Open and download the file\n",
    "os.chdir(r\"C:\\Users\\ruben.crespo\\Downloads\") #C:\\Users\\ruben.crespo\\Documents\\03_tests\\FAO\n",
    "zip_file = requests.get(url_deflator, headers=headers)\n",
    "with open(\"Deflators_E_All_Data.zip\", \"wb\") as file:\n",
    "    file.write(zip_file.content)\n",
    "\n",
    "#print all the files  inside the zip to choose which one do we want\n",
    "with zipfile.ZipFile(\"Deflators_E_All_Data.zip\") as zip:\n",
    "    zip.printdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(r\"C:\\Users\\ruben.crespo\\Downloads\") #C:\\Users\\ruben.crespo\\Documents\\03_tests\\FAO\n",
    "#Open the csv we want and see it\n",
    "with zipfile.ZipFile(\"Deflators_E_All_Data.zip\") as zip:\n",
    "    deflator_data = zip.open(\"Deflators_E_All_Data_NOFLAG.csv\") #open the zip file\n",
    "\n",
    "df_d = pd.read_csv(deflator_data, encoding= \"ISO-8859-1\", header=0) #The encoding was giving me problems\n",
    "\n",
    "# df_d.head()\n",
    "\n",
    "#transform the table\n",
    "df_d = df_d.drop([\"Area Code\", \"Item Code\", \"Element Code\", \"Element\"], axis = 1) #axis 1 for columns 0 for rows\n",
    "\n",
    "filter_USD = (df_d['Item'] == 'GDP Deflator') & (df_d['Unit'] == 'US$') #create a filter and use it the the dataframe\n",
    "df_d = df_d.loc[filter_USD]\n",
    "df_d = df_d.drop([\"Item\", \"Unit\"], axis = 1) #Delete columns\n",
    "df_d = df_d.reset_index(drop=True)  #reset index and delete old index\n",
    "# df_r.sort_index(inplace=True) #we sort the data\n",
    "df_d.columns = df_d.columns.str.replace('Y','') #we remove the Y letter\n",
    "\n",
    "\n",
    "#we transform the table to operate in the final one\n",
    "df_d.set_index(['Area'], inplace=True)\n",
    "df_d = df_d.stack() #take the header and make a column with it\n",
    "df_d = df_d.reset_index(level=[0,1])\n",
    "df_d.rename(columns={'level_1': 'Year', 0:'deflator'}, inplace=True) #rename column for year\n",
    "\n",
    "\n",
    "\"\"\"filter the countries\"\"\"\n",
    "Czechoslovakia = (df_d['Area'] == 'Czechoslovakia') & (df_d['Year'] < '1990') #create a filter and use it the the dataframe\n",
    "df_Czechoslovakia = df_d.loc[Czechoslovakia] #create dataframe out of the filter\n",
    "df_Czechia = df_Czechoslovakia.copy() #we use the copy this time because we do no operation\n",
    "df_Czechia['Area'].replace({'Czechoslovakia': 'Czechia'}, inplace=True)\n",
    "df_d.append(df_Czechia, ignore_index=True)\n",
    "\n",
    "df_Slovakia = df_Czechoslovakia.copy()\n",
    "df_Slovakia['Area'].replace({'Czechoslovakia': 'SLovakia'}, inplace=True)\n",
    "df_d.append(df_Slovakia, ignore_index=True)\n",
    "\n",
    "df_d = df_d[df_d.Area != 'Czechoslovakia']\n",
    "\n",
    "Ethiopia_PDR = (df_d['Area'] == 'Ethiopia PDR') & (df_d['Year'] < '1990') #create a filter and use it the the dataframe\n",
    "df_Ethiopia_PDR = df_d.loc[Ethiopia_PDR] #create dataframe out of the filter\n",
    "df_Eritrea = df_Ethiopia_PDR.copy()\n",
    "df_Eritrea['Area'].replace({'Ethiopia PDR': 'Eritrea'}, inplace=True)\n",
    "df_d.append(df_Eritrea, ignore_index=True)\n",
    "\n",
    "df_Ethiopia = df_Ethiopia_PDR.copy()\n",
    "df_Ethiopia['Area'].replace({'Ethiopia PDR': 'Ethiopia'}, inplace=True)\n",
    "df_d.append(df_Ethiopia, ignore_index=True)\n",
    "\n",
    "df_d = df_d[df_d.Area != 'Ethiopia PDR']\n",
    "\n",
    "Sudan_former = (df_d['Area'] == 'Sudan (former)') & (df_d['Year'] < '2008') #create a filter and use it the the dataframe\n",
    "df_Sudan_former = df_d.loc[Sudan_former] #create dataframe out of the filter\n",
    "df_Sudan = df_Sudan_former.copy()\n",
    "df_Sudan['Area'].replace({'Sudan (former)': 'Sudan'}, inplace=True)\n",
    "df_d = df_d.append(df_Sudan, ignore_index=True)\n",
    "\n",
    "df_South_Sudan = df_Sudan_former.copy()\n",
    "df_South_Sudan['Area'].replace({'Sudan (former)': 'South Sudan'}, inplace=True)\n",
    "df_d = df_d.append(df_South_Sudan, ignore_index=True)\n",
    "\n",
    "df_d = df_d[df_d.Area != 'Sudan (former)']\n",
    "\n",
    "Indonesia = (df_d['Area'] == 'Indonesia') & (df_d['Year'] < '2011') #create a filter and use it the the dataframe\n",
    "df_Indonesia_main = df_d.loc[Indonesia] #create dataframe out of the filter\n",
    "df_Timor_Leste = df_Indonesia_main.copy()\n",
    "df_Timor_Leste['Area'].replace({'Indonesia': 'Timor Leste'}, inplace=True)\n",
    "df_d = df_d.append(df_Timor_Leste, ignore_index=True)\n",
    "\n",
    "df_Indonesia = df_Indonesia_main.copy()\n",
    "df_d = df_d[df_d.Area != 'Indonesia'] #we delete the original Indonesia\n",
    "df_d = df_d.append(df_Indonesia, ignore_index=True) #we import the new Indonesia\n",
    "\n",
    "\"\"\"Prepare the data\"\"\"\n",
    "df_d = df_d.sort_values(by=['Area', 'Year']) #sort the values\n",
    "df_d = df_d.reset_index(drop=True)  #reset index and delete old index\n",
    "df_d.set_index(['Area', 'Year'], inplace=True)\n",
    "df_d_s = df_d.squeeze() #We transform the data from Dataframe to Series\n",
    "df_d_s\n",
    "\n",
    "#the table is ready to be exported as csv\n",
    "df_d.to_csv('deflator_data.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#this was, to veryfy if there are duplicate values, if they were, there is something wrong going on at elaborating the tables\n",
    "df_p_duplicated = df_p[df_p.index.duplicated()]\n",
    "df_d_duplicated = df_p_duplicated.reset_index(level=[0,1])\n",
    "df_d_duplicated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monetary Production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get the file parameters\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "\n",
    "url_monetary = 'http://fenixservices.fao.org/faostat/static/bulkdownloads/Prices_E_All_Data.zip'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File Name                                             Modified             Size\n",
      "Prices_E_All_Data.csv                          2021-05-18 11:39:34     31763335\n",
      "Prices_E_All_Data_NOFLAG.csv                   2021-05-18 11:39:38     26125513\n",
      "Prices_E_Flags.csv                             2021-05-18 11:44:58          142\n"
     ]
    }
   ],
   "source": [
    "#Open and download the file\n",
    "os.chdir(r\"C:\\Users\\ruben.crespo\\Downloads\") #C:\\Users\\ruben.crespo\\Documents\\03_tests\\FAO\n",
    "zip_file = requests.get(url_monetary, headers=headers)\n",
    "with open(\"Prices_E_All_Data.zip\", \"wb\") as file:\n",
    "    file.write(zip_file.content)\n",
    "\n",
    "#print all the files  inside the zip to choose which one do we want\n",
    "with zipfile.ZipFile(\"Prices_E_All_Data.zip\") as zip:\n",
    "    zip.printdir()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Agave fibres nes</th>\n",
       "      <th>Almonds, with shell</th>\n",
       "      <th>Anise, badian, fennel, coriander</th>\n",
       "      <th>Apples</th>\n",
       "      <th>Apricots</th>\n",
       "      <th>Areca nuts</th>\n",
       "      <th>Artichokes</th>\n",
       "      <th>Asparagus</th>\n",
       "      <th>Avocados</th>\n",
       "      <th>Bambara beans</th>\n",
       "      <th>...</th>\n",
       "      <th>Vanilla</th>\n",
       "      <th>Vegetables, fresh nes</th>\n",
       "      <th>Vegetables, leguminous nes</th>\n",
       "      <th>Vetches</th>\n",
       "      <th>Walnuts, with shell</th>\n",
       "      <th>Watermelons</th>\n",
       "      <th>Wheat</th>\n",
       "      <th>Wool, greasy</th>\n",
       "      <th>Yams</th>\n",
       "      <th>Yautia (cocoyam)</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Area</th>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Albania</th>\n",
       "      <th>1993</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>461.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>437.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1457.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1994</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>427.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>975.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1995</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>323.6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>161.8</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1996</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>325.4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>334.9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1997</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>253.1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>235.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Zimbabwe</th>\n",
       "      <th>2014</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>466.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2015</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1897.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1009.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>671.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1789.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1007.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1727.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1865.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1158.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>607.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>500.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>621.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3862 rows × 212 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Agave fibres nes  Almonds, with shell  \\\n",
       "Area     Year                                          \n",
       "Albania  1993               NaN                  NaN   \n",
       "         1994               NaN                  NaN   \n",
       "         1995               NaN                  NaN   \n",
       "         1996               NaN                  NaN   \n",
       "         1997               NaN                  NaN   \n",
       "...                         ...                  ...   \n",
       "Zimbabwe 2014               NaN                  NaN   \n",
       "         2015               NaN                  NaN   \n",
       "         2016               NaN                  NaN   \n",
       "         2017               NaN                  NaN   \n",
       "         2018               NaN                  NaN   \n",
       "\n",
       "               Anise, badian, fennel, coriander  Apples  Apricots  Areca nuts  \\\n",
       "Area     Year                                                                   \n",
       "Albania  1993                               NaN   461.0       NaN         NaN   \n",
       "         1994                               NaN     NaN       NaN         NaN   \n",
       "         1995                               NaN   323.6       NaN         NaN   \n",
       "         1996                               NaN   325.4       NaN         NaN   \n",
       "         1997                               NaN   253.1       NaN         NaN   \n",
       "...                                         ...     ...       ...         ...   \n",
       "Zimbabwe 2014                               NaN     NaN       NaN         NaN   \n",
       "         2015                               NaN  1897.0       NaN         NaN   \n",
       "         2016                               NaN  1789.0       NaN         NaN   \n",
       "         2017                               NaN  1865.0       NaN         NaN   \n",
       "         2018                               NaN     NaN       NaN         NaN   \n",
       "\n",
       "               Artichokes  Asparagus  Avocados  Bambara beans  ...  Vanilla  \\\n",
       "Area     Year                                                  ...            \n",
       "Albania  1993         NaN        NaN       NaN            NaN  ...      NaN   \n",
       "         1994         NaN        NaN       NaN            NaN  ...      NaN   \n",
       "         1995         NaN        NaN       NaN            NaN  ...      NaN   \n",
       "         1996         NaN        NaN       NaN            NaN  ...      NaN   \n",
       "         1997         NaN        NaN       NaN            NaN  ...      NaN   \n",
       "...                   ...        ...       ...            ...  ...      ...   \n",
       "Zimbabwe 2014         NaN        NaN       NaN            NaN  ...      NaN   \n",
       "         2015         NaN        NaN    1009.0            NaN  ...      NaN   \n",
       "         2016         NaN        NaN    1007.0            NaN  ...      NaN   \n",
       "         2017         NaN        NaN    1158.0            NaN  ...      NaN   \n",
       "         2018         NaN        NaN       NaN            NaN  ...      NaN   \n",
       "\n",
       "               Vegetables, fresh nes  Vegetables, leguminous nes  Vetches  \\\n",
       "Area     Year                                                               \n",
       "Albania  1993                  437.1                         NaN      NaN   \n",
       "         1994                  427.0                         NaN      NaN   \n",
       "         1995                    NaN                         NaN      NaN   \n",
       "         1996                    NaN                         NaN      NaN   \n",
       "         1997                    NaN                         NaN      NaN   \n",
       "...                              ...                         ...      ...   \n",
       "Zimbabwe 2014                    NaN                         NaN      NaN   \n",
       "         2015                  671.0                         NaN      NaN   \n",
       "         2016                 1727.0                         NaN      NaN   \n",
       "         2017                  607.0                         NaN      NaN   \n",
       "         2018                    NaN                         NaN      NaN   \n",
       "\n",
       "               Walnuts, with shell  Watermelons  Wheat  Wool, greasy  Yams  \\\n",
       "Area     Year                                                                \n",
       "Albania  1993                  NaN          NaN    NaN        1457.6   NaN   \n",
       "         1994                  NaN          NaN    NaN         975.4   NaN   \n",
       "         1995                  NaN          NaN  161.8           NaN   NaN   \n",
       "         1996                  NaN          NaN  334.9           NaN   NaN   \n",
       "         1997                  NaN          NaN  235.0           NaN   NaN   \n",
       "...                            ...          ...    ...           ...   ...   \n",
       "Zimbabwe 2014                  NaN          NaN  466.0           NaN   NaN   \n",
       "         2015                  NaN          NaN  500.0           NaN   NaN   \n",
       "         2016                  NaN          NaN  500.0           NaN   NaN   \n",
       "         2017                  NaN          NaN  500.0           NaN   NaN   \n",
       "         2018                  NaN          NaN  621.5           NaN   NaN   \n",
       "\n",
       "               Yautia (cocoyam)  \n",
       "Area     Year                    \n",
       "Albania  1993               NaN  \n",
       "         1994               NaN  \n",
       "         1995               NaN  \n",
       "         1996               NaN  \n",
       "         1997               NaN  \n",
       "...                         ...  \n",
       "Zimbabwe 2014               NaN  \n",
       "         2015               NaN  \n",
       "         2016               NaN  \n",
       "         2017               NaN  \n",
       "         2018               NaN  \n",
       "\n",
       "[3862 rows x 212 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#info Source: https://samukweku.github.io/data-wrangling-blog/python/pandas/compressed%20data/zip/archived%20data/2020/07/21/Extract-DataFrame-from-Compressed-Data-into-Pandas.html\n",
    "#Open and see the csv we want\n",
    "with zipfile.ZipFile(\"Prices_E_All_Data.zip\") as zip:\n",
    "    monetary_data = zip.open(\"Prices_E_All_Data_NOFLAG.csv\")\n",
    "df_m = pd.read_csv(monetary_data, encoding= \"ISO-8859-1\", header=0) #The encoding was giving me problems\n",
    "\n",
    "#Transform the table\n",
    "df_m = df_m.drop([\"Area Code\", \"Item Code\", \"Element Code\", \"Months Code\"], axis = 1) #axis 1 for columns 0 for rows\n",
    "filter_monetary = (df_m['Months'] == 'Annual value') & (df_m['Unit'] == 'USD') #create a filter and use it the the dataframe\n",
    "df_m = df_m.loc[filter_monetary]\n",
    "\n",
    "df_m = df_m.drop([\"Element\", \"Months\", \"Unit\"], axis = 1) #Delete unnecessary columns\n",
    "\n",
    "df_m = df_m.reset_index(drop=True)  #reset index and delete old index\n",
    "df_m = df_m.rename(columns={'Item': ''}) #take out the Item part as is giving problems\n",
    "df_m.set_index(['Area', ''], inplace=True) #we create from the dataframe a multiindex with these two columns\n",
    "df_m.sort_index(inplace=True) #we sort the data\n",
    "\n",
    "df_m = df_m.stack() #take the header and make a column with it\n",
    "df_m = df_m.unstack(1) #take the 2nd column and make the header \n",
    "df_m = df_m.reset_index(level=[0,1])    #multiindex to single index, indicate the columns\n",
    "# df_m = df_m.drop([\"Item\"], axis = 1) #Delete Item columns\n",
    "df_m['level_1'] = df_m['level_1'].map(lambda x: x.lstrip('Y')) #remove the letter Y from year\n",
    "df_m.rename(columns={'level_1': 'Year'}, inplace=True) #rename column for year\n",
    "# df_m.to_csv('monetary_data.csv', index=False)\n",
    "\n",
    "\"\"\"filter the countries\"\"\"\n",
    "\n",
    "Belgium_Luxembourg = (df_m['Area'] == 'Belgium-Luxembourg') & (df_m['Year'] < '2000') #create a filter and use it the the dataframe. En caso de que vaya lento, hacer un filtro más preciso\n",
    "df_Belgium_Luxembourg = df_m.loc[Belgium_Luxembourg] #create copy dataframe out of the filter\n",
    "df_Belgium = df_Belgium_Luxembourg.copy()\n",
    "df_Belgium['Area'].replace({'Belgium-Luxembourg': 'Belgium'}, inplace=True)\n",
    "df_p.append(df_Belgium, ignore_index=True)\n",
    "\n",
    "df_Luxembourg = df_Belgium_Luxembourg.copy()\n",
    "df_Luxembourg['Area'].replace({'Belgium-Luxembourg': 'Luxembourg'}, inplace=True)\n",
    "df_m = df_m.append(df_Luxembourg, ignore_index=True)\n",
    "\n",
    "df_m = df_m[df_m.Area != 'Belgium-Luxembourg']\n",
    "\n",
    "Czechoslovakia = (df_m['Area'] == 'Czechoslovakia') & (df_m['Year'] == '1991') #create a filter and use it the the dataframe\n",
    "df_Czechoslovakia = df_m.loc[Czechoslovakia] #create dataframe out of the filter\n",
    "df_Czechia = df_Czechoslovakia.copy()\n",
    "df_Czechia['Area'].replace({'Czechoslovakia': 'Czechia'}, inplace=True)\n",
    "df_m.append(df_Czechia, ignore_index=True)\n",
    "\n",
    "df_Slovakia = df_Czechoslovakia.copy()\n",
    "df_Slovakia['Area'].replace({'Czechoslovakia': 'Slovakia'}, inplace=True)\n",
    "df_m = df_m.append(df_Slovakia, ignore_index=True)\n",
    "\n",
    "df_m = df_m[df_m.Area != 'Czechoslovakia']\n",
    "\n",
    "Serbia_and_Montenegro = (df_m['Area'] == 'Serbia and Montenegro') & (df_m['Year'] < '2006') #create a filter and use it the the dataframe\n",
    "df_Serbia_and_Montenegro = df_m.loc[Serbia_and_Montenegro] #create dataframe out of the filter\n",
    "df_Serbia = df_Serbia_and_Montenegro.copy()\n",
    "df_Serbia['Area'].replace({'Serbia and Montenegro': 'Serbia'}, inplace=True)\n",
    "df_m = df_m.append(df_Serbia, ignore_index=True)\n",
    "\n",
    "df_Montenegro = df_Serbia_and_Montenegro.copy()\n",
    "df_Montenegro['Area'].replace({'Serbia and Montenegro': 'Montenegro'}, inplace=True)\n",
    "df_m = df_m.append(df_Montenegro, ignore_index=True)\n",
    "\n",
    "df_m = df_m[df_m.Area != 'Serbia and Montenegro']\n",
    "\n",
    "Ethiopia_PDR = (df_m['Area'] == 'Ethiopia PDR') & (df_m['Year'] < '1993') #create a filter and use it the the dataframe\n",
    "df_Ethiopia_PDR = df_m.loc[Ethiopia_PDR] #create dataframe out of the filter\n",
    "df_Eritrea = df_Ethiopia_PDR.copy()\n",
    "df_Eritrea['Area'].replace({'Ethiopia PDR': 'Eritrea'}, inplace=True)\n",
    "df_m = df_m.append(df_Eritrea, ignore_index=True)\n",
    "\n",
    "df_Ethiopia = df_Ethiopia_PDR.copy()\n",
    "df_Ethiopia['Area'].replace({'Ethiopia PDR': 'Ethiopia'}, inplace=True)\n",
    "df_m = df_m.append(df_Ethiopia, ignore_index=True)\n",
    "\n",
    "df_m = df_m[df_m.Area != 'Ethiopia PDR']\n",
    "\n",
    "Sudan_former = (df_m['Area'] == 'Sudan (former)') & (df_m['Year'] < '2012') #create a filter and use it the the dataframe\n",
    "df_Sudan_former = df_m.loc[Sudan_former] #create dataframe out of the filter\n",
    "df_Sudan = df_Sudan_former.copy()\n",
    "df_Sudan['Area'].replace({'Sudan (former)': 'Sudan'}, inplace=True)\n",
    "df_m = df_m.append(df_Sudan, ignore_index=True)\n",
    "\n",
    "df_South_Sudan = df_Sudan_former.copy()\n",
    "df_South_Sudan['Area'].replace({'Sudan (former)': 'South Sudan'}, inplace=True)\n",
    "df_m = df_m.append(df_South_Sudan, ignore_index=True)\n",
    "\n",
    "df_m = df_m[df_m.Area != 'Sudan (former)']\n",
    "\n",
    "Indonesia = (df_m['Area'] == 'Indonesia') & (df_m['Year'] < '2020') #create a filter and use it the the dataframe\n",
    "df_Indonesia_main = df_m.loc[Indonesia] #create dataframe out of the filter\n",
    "df_Timor_Leste = df_Indonesia_main.copy()\n",
    "df_Timor_Leste['Area'].replace({'Indonesia': 'Timor Leste'}, inplace=True)\n",
    "df_m = df_m.append(df_Timor_Leste, ignore_index=True)\n",
    "\n",
    "df_Indonesia = df_Indonesia_main.copy()\n",
    "df_m = df_m[df_m.Area != 'Indonesia'] #we delete the original Indonesia\n",
    "df_m = df_m.append(df_Indonesia, ignore_index=True) #we import the new Indonesia\n",
    "\n",
    "\"\"\"Prepare the data\"\"\"\n",
    "\n",
    "df_m = df_m.sort_values(by=['Area', 'Year'])\n",
    "df_m = df_m.reset_index(drop=True)  #reset index and delete old index\n",
    "#since we want to do all in python, we prepare the table for the final operation as a multyindex\n",
    "df_m.set_index(['Area', 'Year'], inplace=True)\n",
    "#the table is ready to be exported as csv\n",
    "df_m.to_csv('monetary_data.csv', index=False)\n",
    "df_m"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculations for the final table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Agave fibres nes</th>\n",
       "      <th>Almonds, with shell</th>\n",
       "      <th>Anise, badian, fennel, coriander</th>\n",
       "      <th>Apples</th>\n",
       "      <th>Apricots</th>\n",
       "      <th>Areca nuts</th>\n",
       "      <th>Artichokes</th>\n",
       "      <th>Asparagus</th>\n",
       "      <th>Avocados</th>\n",
       "      <th>Bambara beans</th>\n",
       "      <th>...</th>\n",
       "      <th>Walnuts, with shell</th>\n",
       "      <th>Watermelons</th>\n",
       "      <th>Wheat</th>\n",
       "      <th>Whey, condensed</th>\n",
       "      <th>Whey, dry</th>\n",
       "      <th>Wine</th>\n",
       "      <th>Wool, greasy</th>\n",
       "      <th>Yams</th>\n",
       "      <th>Yautia (cocoyam)</th>\n",
       "      <th>Yoghurt</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Area</th>\n",
       "      <th>Year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Afghanistan</th>\n",
       "      <th>1961</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1962</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1963</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1964</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1965</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"5\" valign=\"top\">Zimbabwe</th>\n",
       "      <th>2015</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.510807</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.684836</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>90.604000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.583817</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.749677</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>82.238819</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2017</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3.337341</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.475394</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>73.984540</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2018</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>65.751426</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2019</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>14225 rows × 305 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Agave fibres nes  Almonds, with shell  \\\n",
       "Area        Year                                          \n",
       "Afghanistan 1961               NaN                  NaN   \n",
       "            1962               NaN                  NaN   \n",
       "            1963               NaN                  NaN   \n",
       "            1964               NaN                  NaN   \n",
       "            1965               NaN                  NaN   \n",
       "...                            ...                  ...   \n",
       "Zimbabwe    2015               NaN                  NaN   \n",
       "            2016               NaN                  NaN   \n",
       "            2017               NaN                  NaN   \n",
       "            2018               NaN                  NaN   \n",
       "            2019               NaN                  NaN   \n",
       "\n",
       "                  Anise, badian, fennel, coriander    Apples  Apricots  \\\n",
       "Area        Year                                                         \n",
       "Afghanistan 1961                               NaN       NaN       NaN   \n",
       "            1962                               NaN       NaN       NaN   \n",
       "            1963                               NaN       NaN       NaN   \n",
       "            1964                               NaN       NaN       NaN   \n",
       "            1965                               NaN       NaN       NaN   \n",
       "...                                            ...       ...       ...   \n",
       "Zimbabwe    2015                               NaN  3.510807       NaN   \n",
       "            2016                               NaN  3.583817       NaN   \n",
       "            2017                               NaN  3.337341       NaN   \n",
       "            2018                               NaN       NaN       NaN   \n",
       "            2019                               NaN       NaN       NaN   \n",
       "\n",
       "                  Areca nuts  Artichokes  Asparagus  Avocados  Bambara beans  \\\n",
       "Area        Year                                                               \n",
       "Afghanistan 1961         NaN         NaN        NaN       NaN            NaN   \n",
       "            1962         NaN         NaN        NaN       NaN            NaN   \n",
       "            1963         NaN         NaN        NaN       NaN            NaN   \n",
       "            1964         NaN         NaN        NaN       NaN            NaN   \n",
       "            1965         NaN         NaN        NaN       NaN            NaN   \n",
       "...                      ...         ...        ...       ...            ...   \n",
       "Zimbabwe    2015         NaN         NaN        NaN  1.684836            NaN   \n",
       "            2016         NaN         NaN        NaN  1.749677            NaN   \n",
       "            2017         NaN         NaN        NaN  2.475394            NaN   \n",
       "            2018         NaN         NaN        NaN       NaN            NaN   \n",
       "            2019         NaN         NaN        NaN       NaN            NaN   \n",
       "\n",
       "                  ...  Walnuts, with shell  Watermelons      Wheat  \\\n",
       "Area        Year  ...                                                \n",
       "Afghanistan 1961  ...                  NaN          NaN        NaN   \n",
       "            1962  ...                  NaN          NaN        NaN   \n",
       "            1963  ...                  NaN          NaN        NaN   \n",
       "            1964  ...                  NaN          NaN        NaN   \n",
       "            1965  ...                  NaN          NaN        NaN   \n",
       "...               ...                  ...          ...        ...   \n",
       "Zimbabwe    2015  ...                  NaN          NaN  90.604000   \n",
       "            2016  ...                  NaN          NaN  82.238819   \n",
       "            2017  ...                  NaN          NaN  73.984540   \n",
       "            2018  ...                  NaN          NaN  65.751426   \n",
       "            2019  ...                  NaN          NaN        NaN   \n",
       "\n",
       "                  Whey, condensed  Whey, dry  Wine  Wool, greasy  Yams  \\\n",
       "Area        Year                                                         \n",
       "Afghanistan 1961              NaN        NaN   NaN           NaN   NaN   \n",
       "            1962              NaN        NaN   NaN           NaN   NaN   \n",
       "            1963              NaN        NaN   NaN           NaN   NaN   \n",
       "            1964              NaN        NaN   NaN           NaN   NaN   \n",
       "            1965              NaN        NaN   NaN           NaN   NaN   \n",
       "...                           ...        ...   ...           ...   ...   \n",
       "Zimbabwe    2015              NaN        NaN   NaN           NaN   NaN   \n",
       "            2016              NaN        NaN   NaN           NaN   NaN   \n",
       "            2017              NaN        NaN   NaN           NaN   NaN   \n",
       "            2018              NaN        NaN   NaN           NaN   NaN   \n",
       "            2019              NaN        NaN   NaN           NaN   NaN   \n",
       "\n",
       "                  Yautia (cocoyam)  Yoghurt  \n",
       "Area        Year                             \n",
       "Afghanistan 1961               NaN      NaN  \n",
       "            1962               NaN      NaN  \n",
       "            1963               NaN      NaN  \n",
       "            1964               NaN      NaN  \n",
       "            1965               NaN      NaN  \n",
       "...                            ...      ...  \n",
       "Zimbabwe    2015               NaN      NaN  \n",
       "            2016               NaN      NaN  \n",
       "            2017               NaN      NaN  \n",
       "            2018               NaN      NaN  \n",
       "            2019               NaN      NaN  \n",
       "\n",
       "[14225 rows x 305 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#time to calculate\n",
    "\"\"\"USD@2015 prices = LCU prices * 100 / LCU deflator@2015 /Exchange rate for USD in 2015\"\"\"\n",
    "\n",
    "# df_usd = df_p * 100 / df_d / df_m #it does not work\n",
    "\n",
    "df_usd = df_p * 100 / df_m #this works\n",
    "#df_m_s is a series type data, we operate by rows\n",
    "df_final = df_usd.div(df_d_s,  axis = \"rows\",  fill_value = None)\n",
    "\n",
    "\n",
    "df_final\n",
    "#We undo the multiindex (if we want)\n",
    "# df_usd = df_usd.reset_index(drop=True)\n",
    "# df_usd = df_usd.reset_index(level=[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_p_list = list(df_p)\n",
    "df_m_list = list(df_m)\n",
    "#https://www.programiz.com/python-programming/methods/set/symmetric_difference\n",
    "#these functions neen set type data imports\n",
    "symetric_difference = list(set(df_p_list).symmetric_difference(set(df_m_list))) #these are the different values from both\n",
    "df_p_list_difference = list(set(df_p_list).difference(set(df_m_list))) \n",
    "df_m_list_difference = list(set(df_m_list).difference(set(df_p_list)))\n",
    "intersection = list(set(df_m_list).intersection(set(df_p_list)))\n",
    "# df_m_list_difference = df_m_list.difference(df_p_list)\n",
    "\n",
    "\"\"\"\n",
    "total_p: 283\n",
    "total_m: 212\n",
    "equal: 190\n",
    "different_total: 115\n",
    "different production: 93\n",
    "different monetary: 22\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other stuff, put it into the cheat sheet so we don't forget"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prepare the Year column as a Series data type\n",
    "df_transposed = df.T #rows to columns\n",
    "df_transposed[0] = df_transposed[0].map(lambda x: x.lstrip('Y')) #remove the letter Y\n",
    "year = df_transposed[0].drop([0,1,2,3,4,5,6]) #drop first 6 rows, there are 59 years\n",
    "all_years = [year] * 246 #number of countries in list\n",
    "year_concat = pd.concat(all_years) #concat the years\n",
    "year_series = year_concat.reset_index(drop=True)  #reset index and delete old index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Prepare the country column, It has regions bigger than country, but we will filter them at the end\n",
    "countries = df[1].drop([0])\n",
    "countries.head() #It is a Series\n",
    "unique_countries = countries.unique() #is an array\n",
    "#print(len(country.unique())) #246 unique elements\n",
    "#print(len(country)) #51510 total elements\n",
    "country_list = []\n",
    "for country in unique_countries:\n",
    "    country_multiplier = [country] * 59\n",
    "    country_list.extend(country_multiplier)\n",
    "\n",
    "country_series = pd.Series(country_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#product header. We will create a list out of the product\n",
    "product = df[3].drop([0])\n",
    "print(len(product)) #175 unique elements\n",
    "product_array = product.unique()\n",
    "product_list = product_array.tolist()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "New problem with the production, they mixed the Livestock with the crop production, so we have to filter out everything that is not crop. In the Element column, the \"area harvested\" value takes all the crop production values, we will take that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#product crop header. We will filter the livestock and create a list out of the product\n",
    "area_harvested = df[5]=='Area harvested'\n",
    "crops_df = df[area_harvested]\n",
    "product = crops_df[3]\n",
    "print(len(product)) #175 unique elements\n",
    "product_array = product.unique()\n",
    "product_list = product_array.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#final table\n",
    "base_frame = { 'Country': country_series, 'Year': year_series }\n",
    "product_frame = { i : None for i in product_list }\n",
    "\n",
    "base_df = pd.DataFrame(base_frame) #these already have an index in the series\n",
    "product_df = pd.DataFrame(product_frame, index=[0]) #use an index with scalar values\n",
    "template_df = pd.concat([base_df, product_df], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#fill the table with the data of the original table\n",
    "for row_index, row in template_df.loc[1:3].iterrows():\n",
    "    for column_index[0:], value in row.items():\n",
    "        print(\"this is the value: \" + value)\n",
    "        print(row_index)\n",
    "        print(\"this is the column_index: \" + column_index) #column_index es el valor de la cabecera de la columna\n",
    "        print(row) # row es un Series de index por columna y valor de contenido\n",
    "        \n",
    "        # print(value)\n",
    "    # for coklumn_index_base, value_base in row.items():\n",
    "\n",
    "#time\n",
    "\n",
    "# True = template_df[\"Year\"] = df_transposed[0]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for row_index, row in df.loc[0:3].iterrows():\n",
    "    for column_index, value in row.items():\n",
    "        print (value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "# df_row_value = df.loc[1=\"Afghanistan\"][3=\"Apples\"]\n",
    "df_row_value_test = df.loc[1]\n",
    "# df_time = df_row.iloc[\"Y\" + \"1967\"]\n",
    "print (df_row_value_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#GID column. There is no GID data inside the csv, so we have to extract it from somewhere\n",
    "GID_path = r\"C:\\Users\\ruben.crespo\\Documents\\03_tests\\FAO\\ID_extended_gadm.csv\"\n",
    "\n",
    "GID_df = pd.read_csv(GID_path, encoding= \"ISO-8859-1\", header=None) #The encoding was giving me problems\n",
    "\n",
    "GID_df.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geoprocessing",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12 | packaged by conda-forge | (main, Mar 24 2022, 23:18:12) [MSC v.1929 64 bit (AMD64)]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ef1643838ba400d9e56910a4ebe1d7c2ddcc0867ba383b6e8e5b6e0b2b833d6e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
